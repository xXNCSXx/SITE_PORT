{"ast":null,"code":"import { has, isString, isUndefined } from \"../utils/utils\";\nimport { Lexer } from \"./lexer_public\";\nimport { augmentTokenTypes, tokenStructuredMatcher } from \"./tokens\";\nexport function tokenLabel(tokType) {\n  if (hasTokenLabel(tokType)) {\n    return tokType.LABEL;\n  } else {\n    return tokType.name;\n  }\n}\nexport function tokenName(tokType) {\n  return tokType.name;\n}\nexport function hasTokenLabel(obj) {\n  return isString(obj.LABEL) && obj.LABEL !== \"\";\n}\nvar PARENT = \"parent\";\nvar CATEGORIES = \"categories\";\nvar LABEL = \"label\";\nvar GROUP = \"group\";\nvar PUSH_MODE = \"push_mode\";\nvar POP_MODE = \"pop_mode\";\nvar LONGER_ALT = \"longer_alt\";\nvar LINE_BREAKS = \"line_breaks\";\nvar START_CHARS_HINT = \"start_chars_hint\";\nexport function createToken(config) {\n  return createTokenInternal(config);\n}\nfunction createTokenInternal(config) {\n  var pattern = config.pattern;\n  var tokenType = {};\n  tokenType.name = config.name;\n  if (!isUndefined(pattern)) {\n    tokenType.PATTERN = pattern;\n  }\n  if (has(config, PARENT)) {\n    throw \"The parent property is no longer supported.\\n\" + \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\";\n  }\n  if (has(config, CATEGORIES)) {\n    // casting to ANY as this will be fixed inside `augmentTokenTypes``\n    tokenType.CATEGORIES = config[CATEGORIES];\n  }\n  augmentTokenTypes([tokenType]);\n  if (has(config, LABEL)) {\n    tokenType.LABEL = config[LABEL];\n  }\n  if (has(config, GROUP)) {\n    tokenType.GROUP = config[GROUP];\n  }\n  if (has(config, POP_MODE)) {\n    tokenType.POP_MODE = config[POP_MODE];\n  }\n  if (has(config, PUSH_MODE)) {\n    tokenType.PUSH_MODE = config[PUSH_MODE];\n  }\n  if (has(config, LONGER_ALT)) {\n    tokenType.LONGER_ALT = config[LONGER_ALT];\n  }\n  if (has(config, LINE_BREAKS)) {\n    tokenType.LINE_BREAKS = config[LINE_BREAKS];\n  }\n  if (has(config, START_CHARS_HINT)) {\n    tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n  }\n  return tokenType;\n}\nexport var EOF = createToken({\n  name: \"EOF\",\n  pattern: Lexer.NA\n});\naugmentTokenTypes([EOF]);\nexport function createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n  return {\n    image: image,\n    startOffset: startOffset,\n    endOffset: endOffset,\n    startLine: startLine,\n    endLine: endLine,\n    startColumn: startColumn,\n    endColumn: endColumn,\n    tokenTypeIdx: tokType.tokenTypeIdx,\n    tokenType: tokType\n  };\n}\nexport function tokenMatcher(token, tokType) {\n  return tokenStructuredMatcher(token, tokType);\n}","map":{"version":3,"sources":["../../../src/scan/tokens_public.ts"],"names":[],"mappings":"AAAA,SAAS,GAAG,EAAE,QAAQ,EAAE,WAAW,QAAQ,gBAAgB;AAC3D,SAAS,KAAK,QAAQ,gBAAgB;AACtC,SAAS,iBAAiB,EAAE,sBAAsB,QAAQ,UAAU;AAGpE,OAAM,SAAU,UAAU,CAAC,OAAkB,EAAA;EAC3C,IAAI,aAAa,CAAC,OAAO,CAAC,EAAE;IAC1B,OAAO,OAAO,CAAC,KAAK;GACrB,MAAM;IACL,OAAO,OAAO,CAAC,IAAI;EACpB;AACH;AAEA,OAAM,SAAU,SAAS,CAAC,OAAkB,EAAA;EAC1C,OAAO,OAAO,CAAC,IAAI;AACrB;AAEA,OAAM,SAAU,aAAa,CAAC,GAAc,EAAA;EAC1C,OAAO,QAAQ,CAAO,GAAI,CAAC,KAAK,CAAC,IAAU,GAAI,CAAC,KAAK,KAAK,EAAE;AAC9D;AAEA,IAAM,MAAM,GAAG,QAAQ;AACvB,IAAM,UAAU,GAAG,YAAY;AAC/B,IAAM,KAAK,GAAG,OAAO;AACrB,IAAM,KAAK,GAAG,OAAO;AACrB,IAAM,SAAS,GAAG,WAAW;AAC7B,IAAM,QAAQ,GAAG,UAAU;AAC3B,IAAM,UAAU,GAAG,YAAY;AAC/B,IAAM,WAAW,GAAG,aAAa;AACjC,IAAM,gBAAgB,GAAG,kBAAkB;AAE3C,OAAM,SAAU,WAAW,CAAC,MAAoB,EAAA;EAC9C,OAAO,mBAAmB,CAAC,MAAM,CAAC;AACpC;AAEA,SAAS,mBAAmB,CAAC,MAAoB,EAAA;EAC/C,IAAI,OAAO,GAAG,MAAM,CAAC,OAAO;EAE5B,IAAI,SAAS,GAAmB,CAAA,CAAE;EAClC,SAAS,CAAC,IAAI,GAAG,MAAM,CAAC,IAAI;EAE5B,IAAI,CAAC,WAAW,CAAC,OAAO,CAAC,EAAE;IACzB,SAAS,CAAC,OAAO,GAAG,OAAO;EAC5B;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,MAAM,CAAC,EAAE;IACvB,MACE,+CAA+C,GAC/C,8FAA8F;EAEjG;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,UAAU,CAAC,EAAE;IAC3B;IACA,SAAS,CAAC,UAAU,GAAQ,MAAM,CAAC,UAAU,CAAC;EAC/C;EAED,iBAAiB,CAAC,CAAC,SAAS,CAAC,CAAC;EAE9B,IAAI,GAAG,CAAC,MAAM,EAAE,KAAK,CAAC,EAAE;IACtB,SAAS,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,CAAC;EAChC;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,KAAK,CAAC,EAAE;IACtB,SAAS,CAAC,KAAK,GAAG,MAAM,CAAC,KAAK,CAAC;EAChC;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,QAAQ,CAAC,EAAE;IACzB,SAAS,CAAC,QAAQ,GAAG,MAAM,CAAC,QAAQ,CAAC;EACtC;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,SAAS,CAAC,EAAE;IAC1B,SAAS,CAAC,SAAS,GAAG,MAAM,CAAC,SAAS,CAAC;EACxC;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,UAAU,CAAC,EAAE;IAC3B,SAAS,CAAC,UAAU,GAAG,MAAM,CAAC,UAAU,CAAC;EAC1C;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,WAAW,CAAC,EAAE;IAC5B,SAAS,CAAC,WAAW,GAAG,MAAM,CAAC,WAAW,CAAC;EAC5C;EAED,IAAI,GAAG,CAAC,MAAM,EAAE,gBAAgB,CAAC,EAAE;IACjC,SAAS,CAAC,gBAAgB,GAAG,MAAM,CAAC,gBAAgB,CAAC;EACtD;EAED,OAAO,SAAS;AAClB;AAEA,OAAO,IAAM,GAAG,GAAG,WAAW,CAAC;EAAE,IAAI,EAAE,KAAK;EAAE,OAAO,EAAE,KAAK,CAAC;AAAE,CAAE,CAAC;AAClE,iBAAiB,CAAC,CAAC,GAAG,CAAC,CAAC;AAExB,OAAM,SAAU,mBAAmB,CACjC,OAAkB,EAClB,KAAa,EACb,WAAmB,EACnB,SAAiB,EACjB,SAAiB,EACjB,OAAe,EACf,WAAmB,EACnB,SAAiB,EAAA;EAEjB,OAAO;IACL,KAAK,EAAA,KAAA;IACL,WAAW,EAAA,WAAA;IACX,SAAS,EAAA,SAAA;IACT,SAAS,EAAA,SAAA;IACT,OAAO,EAAA,OAAA;IACP,WAAW,EAAA,WAAA;IACX,SAAS,EAAA,SAAA;IACT,YAAY,EAAQ,OAAQ,CAAC,YAAY;IACzC,SAAS,EAAE;GACZ;AACH;AAEA,OAAM,SAAU,YAAY,CAAC,KAAa,EAAE,OAAkB,EAAA;EAC5D,OAAO,sBAAsB,CAAC,KAAK,EAAE,OAAO,CAAC;AAC/C","sourceRoot":"","sourcesContent":["import { has, isString, isUndefined } from \"../utils/utils\";\nimport { Lexer } from \"./lexer_public\";\nimport { augmentTokenTypes, tokenStructuredMatcher } from \"./tokens\";\nexport function tokenLabel(tokType) {\n    if (hasTokenLabel(tokType)) {\n        return tokType.LABEL;\n    }\n    else {\n        return tokType.name;\n    }\n}\nexport function tokenName(tokType) {\n    return tokType.name;\n}\nexport function hasTokenLabel(obj) {\n    return isString(obj.LABEL) && obj.LABEL !== \"\";\n}\nvar PARENT = \"parent\";\nvar CATEGORIES = \"categories\";\nvar LABEL = \"label\";\nvar GROUP = \"group\";\nvar PUSH_MODE = \"push_mode\";\nvar POP_MODE = \"pop_mode\";\nvar LONGER_ALT = \"longer_alt\";\nvar LINE_BREAKS = \"line_breaks\";\nvar START_CHARS_HINT = \"start_chars_hint\";\nexport function createToken(config) {\n    return createTokenInternal(config);\n}\nfunction createTokenInternal(config) {\n    var pattern = config.pattern;\n    var tokenType = {};\n    tokenType.name = config.name;\n    if (!isUndefined(pattern)) {\n        tokenType.PATTERN = pattern;\n    }\n    if (has(config, PARENT)) {\n        throw (\"The parent property is no longer supported.\\n\" +\n            \"See: https://github.com/chevrotain/chevrotain/issues/564#issuecomment-349062346 for details.\");\n    }\n    if (has(config, CATEGORIES)) {\n        // casting to ANY as this will be fixed inside `augmentTokenTypes``\n        tokenType.CATEGORIES = config[CATEGORIES];\n    }\n    augmentTokenTypes([tokenType]);\n    if (has(config, LABEL)) {\n        tokenType.LABEL = config[LABEL];\n    }\n    if (has(config, GROUP)) {\n        tokenType.GROUP = config[GROUP];\n    }\n    if (has(config, POP_MODE)) {\n        tokenType.POP_MODE = config[POP_MODE];\n    }\n    if (has(config, PUSH_MODE)) {\n        tokenType.PUSH_MODE = config[PUSH_MODE];\n    }\n    if (has(config, LONGER_ALT)) {\n        tokenType.LONGER_ALT = config[LONGER_ALT];\n    }\n    if (has(config, LINE_BREAKS)) {\n        tokenType.LINE_BREAKS = config[LINE_BREAKS];\n    }\n    if (has(config, START_CHARS_HINT)) {\n        tokenType.START_CHARS_HINT = config[START_CHARS_HINT];\n    }\n    return tokenType;\n}\nexport var EOF = createToken({ name: \"EOF\", pattern: Lexer.NA });\naugmentTokenTypes([EOF]);\nexport function createTokenInstance(tokType, image, startOffset, endOffset, startLine, endLine, startColumn, endColumn) {\n    return {\n        image: image,\n        startOffset: startOffset,\n        endOffset: endOffset,\n        startLine: startLine,\n        endLine: endLine,\n        startColumn: startColumn,\n        endColumn: endColumn,\n        tokenTypeIdx: tokType.tokenTypeIdx,\n        tokenType: tokType\n    };\n}\nexport function tokenMatcher(token, tokType) {\n    return tokenStructuredMatcher(token, tokType);\n}\n//# sourceMappingURL=tokens_public.js.map"]},"metadata":{},"sourceType":"module"}